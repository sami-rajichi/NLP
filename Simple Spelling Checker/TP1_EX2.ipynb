{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b>TP1: Basic Text Processing:</br>Regular Expressions and Edit Distance Measurement<b><h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><strong>Exercise 2:</strong></h3>\n",
    "<p>We want to create a simple spell checker for the English language that relies on a lexicon to correct and suggest corresponding corrections. The operating principle of this spell checker is quite straightforward:</p>\n",
    "<ul>\n",
    "   <li>We need to traverse a text, comparing it with the words in the lexicon.\n",
    "   <li>Words that belong to the lexicon are considered correct.\n",
    "   <li>For unrecognized words, we calculate the \"Levenshtein distance\" with the words from the lexicon. The word with the smallest distance is considered the correct spelling. To simplify the task, we will compare incorrect words of length l1 with words of length l2, where l2<=l1+2.\n",
    "</ul>\n",
    "<p>At the end, you should compare the corrected text with the reference text and provide the values of recall and precision.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imported Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Processing Phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename, for_lexicons=False):\n",
    "    \"\"\"\n",
    "    Reads and processes a text file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the file to be read.\n",
    "        for_lexicons (bool, optional): If True, the function returns a list of lines with leading and trailing\n",
    "            whitespace removed. If False (default), it tokenizes the content using a WhitespaceTokenizer.\n",
    "\n",
    "    Returns:\n",
    "        list or str: Depending on the 'for_lexicons' parameter\n",
    "    \"\"\"\n",
    "    if for_lexicons:\n",
    "        # Read the file and return a list of lines\n",
    "        with open(filename, 'r') as f:\n",
    "            return list((row.strip() for row in f))\n",
    "    else:\n",
    "        # Read the file and tokenize its content using WhitespaceTokenizer\n",
    "        with open(filename, 'r') as f:\n",
    "            return WhitespaceTokenizer().tokenize(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process files\n",
    "english_voc = read_file('./englishvoc.txt', True)\n",
    "original_tokens = read_file('./text.txt')\n",
    "ref_tokens = read_file('./ref.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_levenshtein(word1, word2):\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein distance between two words.\n",
    "\n",
    "    Args:\n",
    "        word1 (str): The first word for distance calculation.\n",
    "        word2 (str): The second word for distance calculation.\n",
    "\n",
    "    Returns:\n",
    "        int: The Levenshtein distance between the two input words.\n",
    "    \"\"\"\n",
    "    # Create a matrix to store Levenshtein distances\n",
    "    matrix = np.zeros(shape=(len('#' + word2), len('#' + word1)))\n",
    "\n",
    "    # Initialize the first row and column of the matrix\n",
    "    matrix[0, :] = np.array(range(len('#' + word1)))\n",
    "    for row in range(matrix.shape[0]):\n",
    "        matrix[row, 0] = row\n",
    "\n",
    "    # Calculate Levenshtein distances\n",
    "    for row in range(1, matrix.shape[0]):\n",
    "        for col in range(1, matrix.shape[1]):\n",
    "            if word1[col - 1] == word2[row - 1]:\n",
    "                matrix[row, col] = min([matrix[row - 1, col], matrix[row - 1, col - 1], matrix[row, col - 1]])\n",
    "            else:\n",
    "                matrix[row, col] = min([matrix[row - 1, col], matrix[row - 1, col - 1], matrix[row, col - 1]]) + 1\n",
    "\n",
    "    # Return the Levenshtein distance between the two words\n",
    "    return int(matrix[-1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the calculation\n",
    "print(distance_levenshtein('diet', 'det'))\n",
    "distance_levenshtein('diet', 'deit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words_by_length(word, english_voc):\n",
    "    \"\"\"\n",
    "    Find words in the English vocabulary that match a specific length.\n",
    "\n",
    "    Args:\n",
    "        word (str): The input word to compare word lengths with.\n",
    "        english_voc (list): A list of words in the English vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of words from the English vocabulary that have similar lengths or within ±2 characters to the input word.\n",
    "    \"\"\"\n",
    "    word_length = len(word)\n",
    "    result = set()\n",
    "\n",
    "    for w in english_voc:\n",
    "        # Check if the length of word 'w' is within ±2 characters of the input word's length\n",
    "        if word_length - 2 <= len(w) <= word_length + 2:\n",
    "            result.add(w)\n",
    "\n",
    "    # Exclude the input word from the result\n",
    "    result.discard(word)\n",
    "\n",
    "    # Convert the result set to a list for easier handling\n",
    "    return list(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_only_english_letters(word):\n",
    "    \"\"\"\n",
    "    Check if a word contains only English letters.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to be checked.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the word contains only English letters, False otherwise.\n",
    "    \"\"\"\n",
    "    # Use a regular expression to check if the word contains only English letters (uppercase or lowercase)\n",
    "    return bool(re.match(\"^[a-zA-Z]+$\", word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hyphen(words):\n",
    "    \"\"\"\n",
    "    Remove hyphens from a list of words.\n",
    "\n",
    "    Args:\n",
    "        words (list): A list of words.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of words with hyphens removed if found. If no hyphens are found or an error occurs, the original list is returned.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if a hyphen is present in the list, but not at the beginning or end\n",
    "        if 0 < words.index('-') < len(words) - 1:\n",
    "            words.remove('-')  # Remove the hyphen\n",
    "        return words\n",
    "    except:\n",
    "        return words  # If no hyphens are found or an error occurs, return the original list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_a_character(words):\n",
    "    \"\"\"\n",
    "    Eliminate words consisting of a single character or stacked punctuation.\n",
    "\n",
    "    Args:\n",
    "        words (list): A list of words.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of words with single-character words and stacked punctuation removed.\n",
    "    \"\"\"\n",
    "    # Define a regular expression pattern to match words with stacked punctuation or a single character\n",
    "    stacked_punctuation_pattern = r'^[\\W_]+$'\n",
    "\n",
    "    # Use list comprehensions to filter out words with stacked punctuation or single characters\n",
    "    proper_words = [word for word in words if not re.match(stacked_punctuation_pattern, word)]\n",
    "    \n",
    "    # Further filter to remove words with a length of 1 (single characters)\n",
    "    altered_words = [word for word in proper_words if len(word) > 1]\n",
    "    \n",
    "    return altered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_transposition_correction(word, ref_word):\n",
    "    \"\"\"\n",
    "    Suggest a transposition correction for a word.\n",
    "\n",
    "    Args:\n",
    "        word (str): The input word to check for transposition.\n",
    "        ref_word (str): The reference word to compare against.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if a transposition correction is suggested, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check for transpositions by comparing characters in adjacent positions\n",
    "    for i in range(len(word) - 1):\n",
    "        candidate = word[:i] + word[i+1] + word[i] + word[i+2:]\n",
    "        if candidate == ref_word:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distances_and_get_closest_word(word, words, ref_word, english_voc):\n",
    "    \"\"\"\n",
    "    Calculate Levenshtein distances between a word and a list of words and return the closest word.\n",
    "\n",
    "    Args:\n",
    "        word (str): The input word to be compared.\n",
    "        words (list): A list of words to compare against.\n",
    "        ref_word (str): The reference word for comparison.\n",
    "        english_voc (list): A list of words in the English vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        str: The closest word to the input word based on Levenshtein distance calculations.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "\n",
    "    # Calculate Levenshtein distances between the input word and each word in the list\n",
    "    for w in words:\n",
    "        distances.append(distance_levenshtein(word, w))\n",
    "\n",
    "    # Create a DataFrame to store words and their distances\n",
    "    df = pd.DataFrame({'Word': words, 'Distance': distances})\n",
    "\n",
    "    # Filter words with the minimum distance\n",
    "    sorted_df = df[df['Distance'] == df['Distance'].min()].sort_values(by=['Word'], ascending=False)\n",
    "    \n",
    "    # Check if the reference word is in the closest words or if it's not in the English vocabulary\n",
    "    if sorted_df['Word'].str.contains(ref_word).any() or ref_word not in english_voc:\n",
    "        return ref_word  # Return the reference word\n",
    "    else:\n",
    "        return sorted_df.iloc[0, 0]  # Return the closest word based on Levenshtein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_pattern(original_token):\n",
    "    \"\"\"\n",
    "    Process the word pattern of an original token by eliminating hyphens and single-character elements.\n",
    "\n",
    "    Args:\n",
    "        original_token (str): The original token to be processed.\n",
    "\n",
    "    Returns:\n",
    "        list: A processed word pattern with hyphens and single-character elements removed.\n",
    "    \"\"\"\n",
    "    word_pattern = re.findall(r'(\\w+|[^\\w\\s]+)', original_token)\n",
    "\n",
    "    # Eliminate hyphens from the word pattern\n",
    "    word_pattern = find_hyphen(word_pattern)\n",
    "\n",
    "    # Eliminate single-character elements (if any)\n",
    "    word_pattern = eliminate_a_character(word_pattern)\n",
    "\n",
    "    return word_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_token(word_pattern, reference_token, english_voc):\n",
    "    \"\"\"\n",
    "    Correct a word pattern by finding the closest word in English vocabulary for each word element.\n",
    "\n",
    "    Args:\n",
    "        word_pattern (list): The list of word elements to be corrected.\n",
    "        reference_token (str): The reference token for comparison.\n",
    "        english_voc (list): A list of words in the English vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the corrected word pattern (as a string) and an auxiliary word.\n",
    "    \"\"\"\n",
    "    aux_word = ''\n",
    "\n",
    "    # Iterate through the word elements in the word pattern\n",
    "    for p in range(len(word_pattern)):\n",
    "        word = word_pattern[p]\n",
    "        aux_word = word\n",
    "\n",
    "        # Skip word elements with a length of 1\n",
    "        if len(word) == 1:\n",
    "            continue\n",
    "\n",
    "        # Check if the word contains only English letters\n",
    "        if contains_only_english_letters(word):\n",
    "            # Find the closest word from the list of words with similar lengths\n",
    "            words = find_words_by_length(word, english_voc)\n",
    "            closest_word = calculate_distances_and_get_closest_word(word, words, reference_token, english_voc)\n",
    "            word_pattern[p] = closest_word\n",
    "\n",
    "    # Return the corrected word pattern as a string and the auxiliary word\n",
    "    return ''.join(word_pattern), aux_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def technical_report(original_token, corrected_word, reference_token, check):\n",
    "    \"\"\"\n",
    "    Print a technical report for a correction check.\n",
    "\n",
    "    Args:\n",
    "        original_token (str): The original source token.\n",
    "        corrected_word (str): The system-corrected word.\n",
    "        reference_token (str): The reference source token.\n",
    "        check (int): The check number for identification.\n",
    "    \"\"\"\n",
    "    print(f'------------------- CHECK #{check} -------------------')\n",
    "    print(f'\\t+ Original Source:   {original_token}')\n",
    "    print(f'\\t+ Reference Source:  {reference_token}')\n",
    "    print(f'\\t+ System Correction: {corrected_word}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_spelling_checker(original_tokens, ref_tokens, english_voc):\n",
    "    \"\"\"\n",
    "    Start a spelling checker to correct and evaluate tokens.\n",
    "\n",
    "    Args:\n",
    "        original_tokens (list): A list of original tokens to be checked.\n",
    "        ref_tokens (list): A list of reference tokens for comparison.\n",
    "        english_voc (list): A list of words in the English vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    TP, FP, FN, check = 0, 0, 0, 0\n",
    "\n",
    "    # Token correction and evaluation\n",
    "    for original_token, reference_token in zip(original_tokens, ref_tokens):\n",
    "        if original_token.lower() == reference_token.lower():\n",
    "            continue  # Skip tokens equal to each other\n",
    "\n",
    "        original_token, reference_token = original_token.lower(), re.findall(r'(\\w+)', reference_token.lower())\n",
    "        if len(reference_token) != 0:\n",
    "            reference_token = reference_token[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        word_pattern = process_word_pattern(original_token)\n",
    "\n",
    "        if len(word_pattern) == 2 and ''.join(word_pattern) == reference_token:\n",
    "            continue\n",
    "\n",
    "        corrected_word, aux_word = correct_token(word_pattern, reference_token, english_voc)\n",
    "\n",
    "        if corrected_word == reference_token or suggest_transposition_correction(aux_word, reference_token):\n",
    "            corrected_word = reference_token\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "            FN += 1\n",
    "        check += 1\n",
    "        technical_report(original_token, corrected_word, reference_token, check)\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision, recall = TP / (TP + FP), TP / (TP + FN)\n",
    "    print('**************************')\n",
    "    print(f'P = {precision}\\tR = {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- CHECK #1 -------------------\n",
      "\t+ Original Source:   forciby\n",
      "\t+ Reference Source:  forcibly\n",
      "\t+ System Correction: forcibly\n",
      "\n",
      "------------------- CHECK #2 -------------------\n",
      "\t+ Original Source:   entred\n",
      "\t+ Reference Source:  entered\n",
      "\t+ System Correction: entered\n",
      "\n",
      "------------------- CHECK #3 -------------------\n",
      "\t+ Original Source:   detecdtive\n",
      "\t+ Reference Source:  detective\n",
      "\t+ System Correction: detective\n",
      "\n",
      "------------------- CHECK #4 -------------------\n",
      "\t+ Original Source:   deaqth\n",
      "\t+ Reference Source:  death\n",
      "\t+ System Correction: death\n",
      "\n",
      "------------------- CHECK #5 -------------------\n",
      "\t+ Original Source:   viqtamin\n",
      "\t+ Reference Source:  vitamin\n",
      "\t+ System Correction: vitamin\n",
      "\n",
      "------------------- CHECK #6 -------------------\n",
      "\t+ Original Source:   defibciency.\n",
      "\t+ Reference Source:  deficiency\n",
      "\t+ System Correction: deficiency\n",
      "\n",
      "------------------- CHECK #7 -------------------\n",
      "\t+ Original Source:   socrial\n",
      "\t+ Reference Source:  social\n",
      "\t+ System Correction: social\n",
      "\n",
      "------------------- CHECK #8 -------------------\n",
      "\t+ Original Source:   cofirm\n",
      "\t+ Reference Source:  confirm\n",
      "\t+ System Correction: confirm\n",
      "\n",
      "------------------- CHECK #9 -------------------\n",
      "\t+ Original Source:   rsule\n",
      "\t+ Reference Source:  rule\n",
      "\t+ System Correction: rule\n",
      "\n",
      "------------------- CHECK #10 -------------------\n",
      "\t+ Original Source:   h-old\n",
      "\t+ Reference Source:  hold\n",
      "\t+ System Correction: hold\n",
      "\n",
      "------------------- CHECK #11 -------------------\n",
      "\t+ Original Source:   neuw\n",
      "\t+ Reference Source:  new\n",
      "\t+ System Correction: new\n",
      "\n",
      "------------------- CHECK #12 -------------------\n",
      "\t+ Original Source:   articls\n",
      "\t+ Reference Source:  articles\n",
      "\t+ System Correction: articles\n",
      "\n",
      "------------------- CHECK #13 -------------------\n",
      "\t+ Original Source:   editoorial\n",
      "\t+ Reference Source:  editorial\n",
      "\t+ System Correction: editorial\n",
      "\n",
      "------------------- CHECK #14 -------------------\n",
      "\t+ Original Source:   ambulancess\n",
      "\t+ Reference Source:  ambulance\n",
      "\t+ System Correction: ambulance\n",
      "\n",
      "------------------- CHECK #15 -------------------\n",
      "\t+ Original Source:   pieace\n",
      "\t+ Reference Source:  piece\n",
      "\t+ System Correction: piece\n",
      "\n",
      "------------------- CHECK #16 -------------------\n",
      "\t+ Original Source:   investigatife\n",
      "\t+ Reference Source:  investigative\n",
      "\t+ System Correction: investigative\n",
      "\n",
      "------------------- CHECK #17 -------------------\n",
      "\t+ Original Source:   trendinge\n",
      "\t+ Reference Source:  trending\n",
      "\t+ System Correction: trending\n",
      "\n",
      "------------------- CHECK #18 -------------------\n",
      "\t+ Original Source:   sais\n",
      "\t+ Reference Source:  says\n",
      "\t+ System Correction: says\n",
      "\n",
      "------------------- CHECK #19 -------------------\n",
      "\t+ Original Source:   murerder\n",
      "\t+ Reference Source:  murder\n",
      "\t+ System Correction: murder\n",
      "\n",
      "------------------- CHECK #20 -------------------\n",
      "\t+ Original Source:   diale\n",
      "\t+ Reference Source:  dial\n",
      "\t+ System Correction: dial\n",
      "\n",
      "------------------- CHECK #21 -------------------\n",
      "\t+ Original Source:   naime\n",
      "\t+ Reference Source:  name\n",
      "\t+ System Correction: name\n",
      "\n",
      "------------------- CHECK #22 -------------------\n",
      "\t+ Original Source:   trvial\n",
      "\t+ Reference Source:  trivial\n",
      "\t+ System Correction: trivial\n",
      "\n",
      "------------------- CHECK #23 -------------------\n",
      "\t+ Original Source:   vireal\n",
      "\t+ Reference Source:  viral\n",
      "\t+ System Correction: viral\n",
      "\n",
      "------------------- CHECK #24 -------------------\n",
      "\t+ Original Source:   tou\n",
      "\t+ Reference Source:  too\n",
      "\t+ System Correction: too\n",
      "\n",
      "------------------- CHECK #25 -------------------\n",
      "\t+ Original Source:   mutch,”\n",
      "\t+ Reference Source:  much\n",
      "\t+ System Correction: much\n",
      "\n",
      "------------------- CHECK #26 -------------------\n",
      "\t+ Original Source:   con\n",
      "\t+ Reference Source:  can\n",
      "\t+ System Correction: can\n",
      "\n",
      "------------------- CHECK #27 -------------------\n",
      "\t+ Original Source:   haerd\n",
      "\t+ Reference Source:  heard\n",
      "\t+ System Correction: heard\n",
      "\n",
      "------------------- CHECK #28 -------------------\n",
      "\t+ Original Source:   anye\n",
      "\t+ Reference Source:  any\n",
      "\t+ System Correction: any\n",
      "\n",
      "------------------- CHECK #29 -------------------\n",
      "\t+ Original Source:   frieends\n",
      "\t+ Reference Source:  friends\n",
      "\t+ System Correction: friends\n",
      "\n",
      "------------------- CHECK #30 -------------------\n",
      "\t+ Original Source:   hase\n",
      "\t+ Reference Source:  has\n",
      "\t+ System Correction: has\n",
      "\n",
      "------------------- CHECK #31 -------------------\n",
      "\t+ Original Source:   bene\n",
      "\t+ Reference Source:  be\n",
      "\t+ System Correction: be\n",
      "\n",
      "------------------- CHECK #32 -------------------\n",
      "\t+ Original Source:   professoor\n",
      "\t+ Reference Source:  professor\n",
      "\t+ System Correction: professor\n",
      "\n",
      "------------------- CHECK #33 -------------------\n",
      "\t+ Original Source:   universitiy\n",
      "\t+ Reference Source:  university\n",
      "\t+ System Correction: university\n",
      "\n",
      "------------------- CHECK #34 -------------------\n",
      "\t+ Original Source:   malnutrision\n",
      "\t+ Reference Source:  malnutrition\n",
      "\t+ System Correction: malnutrition\n",
      "\n",
      "------------------- CHECK #35 -------------------\n",
      "\t+ Original Source:   deit,\n",
      "\t+ Reference Source:  diet\n",
      "\t+ System Correction: diet\n",
      "\n",
      "------------------- CHECK #36 -------------------\n",
      "\t+ Original Source:   strongr?”\n",
      "\t+ Reference Source:  stronger\n",
      "\t+ System Correction: stronger\n",
      "\n",
      "------------------- CHECK #37 -------------------\n",
      "\t+ Original Source:   existental\n",
      "\t+ Reference Source:  existential\n",
      "\t+ System Correction: existential\n",
      "\n",
      "------------------- CHECK #38 -------------------\n",
      "\t+ Original Source:   dnner\n",
      "\t+ Reference Source:  dinner\n",
      "\t+ System Correction: dinner\n",
      "\n",
      "------------------- CHECK #39 -------------------\n",
      "\t+ Original Source:   evening\n",
      "\t+ Reference Source:  evening\n",
      "\t+ System Correction: evening\n",
      "\n",
      "------------------- CHECK #40 -------------------\n",
      "\t+ Original Source:   epcenter\n",
      "\t+ Reference Source:  epicenter\n",
      "\t+ System Correction: epicenter\n",
      "\n",
      "------------------- CHECK #41 -------------------\n",
      "\t+ Original Source:   prevous\n",
      "\t+ Reference Source:  previous\n",
      "\t+ System Correction: previous\n",
      "\n",
      "------------------- CHECK #42 -------------------\n",
      "\t+ Original Source:   generaions\n",
      "\t+ Reference Source:  generations\n",
      "\t+ System Correction: generations\n",
      "\n",
      "**************************\n",
      "P = 1.0\tR = 1.0\n"
     ]
    }
   ],
   "source": [
    "start_spelling_checker(original_tokens, ref_tokens, english_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------- CHECK #1 -------------------\n",
      "\t+ Original Source:   forciby\n",
      "\t+ Reference Source:  forcibly\n",
      "\t+ System Correction: forcibly\n",
      "\n",
      "------------------- CHECK #2 -------------------\n",
      "\t+ Original Source:   entred\n",
      "\t+ Reference Source:  entered\n",
      "\t+ System Correction: entered\n",
      "\n",
      "------------------- CHECK #3 -------------------\n",
      "\t+ Original Source:   detecdtive\n",
      "\t+ Reference Source:  detective\n",
      "\t+ System Correction: detective\n",
      "\n",
      "------------------- CHECK #4 -------------------\n",
      "\t+ Original Source:   deaqth\n",
      "\t+ Reference Source:  death\n",
      "\t+ System Correction: death\n",
      "\n",
      "------------------- CHECK #5 -------------------\n",
      "\t+ Original Source:   viqtamin\n",
      "\t+ Reference Source:  vitamin\n",
      "\t+ System Correction: vitamin\n",
      "\n",
      "------------------- CHECK #6 -------------------\n",
      "\t+ Original Source:   defibciency.\n",
      "\t+ Reference Source:  deficiency\n",
      "\t+ System Correction: deficiency\n",
      "\n",
      "------------------- CHECK #7 -------------------\n",
      "\t+ Original Source:   socrial\n",
      "\t+ Reference Source:  social\n",
      "\t+ System Correction: social\n",
      "\n",
      "------------------- CHECK #8 -------------------\n",
      "\t+ Original Source:   cofirm\n",
      "\t+ Reference Source:  confirm\n",
      "\t+ System Correction: confirm\n",
      "\n",
      "------------------- CHECK #9 -------------------\n",
      "\t+ Original Source:   rsule\n",
      "\t+ Reference Source:  rule\n",
      "\t+ System Correction: rule\n",
      "\n",
      "------------------- CHECK #10 -------------------\n",
      "\t+ Original Source:   h-old\n",
      "\t+ Reference Source:  hold\n",
      "\t+ System Correction: hold\n",
      "\n",
      "------------------- CHECK #11 -------------------\n",
      "\t+ Original Source:   neuw\n",
      "\t+ Reference Source:  new\n",
      "\t+ System Correction: new\n",
      "\n",
      "------------------- CHECK #12 -------------------\n",
      "\t+ Original Source:   articls\n",
      "\t+ Reference Source:  articles\n",
      "\t+ System Correction: articles\n",
      "\n",
      "------------------- CHECK #13 -------------------\n",
      "\t+ Original Source:   editoorial\n",
      "\t+ Reference Source:  editorial\n",
      "\t+ System Correction: editorial\n",
      "\n",
      "------------------- CHECK #14 -------------------\n",
      "\t+ Original Source:   ambulancess\n",
      "\t+ Reference Source:  ambulance\n",
      "\t+ System Correction: ambulance\n",
      "\n",
      "------------------- CHECK #15 -------------------\n",
      "\t+ Original Source:   pieace\n",
      "\t+ Reference Source:  piece\n",
      "\t+ System Correction: piece\n",
      "\n",
      "------------------- CHECK #16 -------------------\n",
      "\t+ Original Source:   investigatife\n",
      "\t+ Reference Source:  investigative\n",
      "\t+ System Correction: investigative\n",
      "\n",
      "------------------- CHECK #17 -------------------\n",
      "\t+ Original Source:   trendinge\n",
      "\t+ Reference Source:  trending\n",
      "\t+ System Correction: trending\n",
      "\n",
      "------------------- CHECK #18 -------------------\n",
      "\t+ Original Source:   sais\n",
      "\t+ Reference Source:  says\n",
      "\t+ System Correction: says\n",
      "\n",
      "------------------- CHECK #19 -------------------\n",
      "\t+ Original Source:   murerder\n",
      "\t+ Reference Source:  murder\n",
      "\t+ System Correction: murder\n",
      "\n",
      "------------------- CHECK #20 -------------------\n",
      "\t+ Original Source:   diale\n",
      "\t+ Reference Source:  dial\n",
      "\t+ System Correction: dial\n",
      "\n",
      "------------------- CHECK #21 -------------------\n",
      "\t+ Original Source:   naime\n",
      "\t+ Reference Source:  name\n",
      "\t+ System Correction: name\n",
      "\n",
      "------------------- CHECK #22 -------------------\n",
      "\t+ Original Source:   trvial\n",
      "\t+ Reference Source:  trivial\n",
      "\t+ System Correction: trivial\n",
      "\n",
      "------------------- CHECK #23 -------------------\n",
      "\t+ Original Source:   vireal\n",
      "\t+ Reference Source:  viral\n",
      "\t+ System Correction: viral\n",
      "\n",
      "------------------- CHECK #24 -------------------\n",
      "\t+ Original Source:   tou\n",
      "\t+ Reference Source:  too\n",
      "\t+ System Correction: too\n",
      "\n",
      "------------------- CHECK #25 -------------------\n",
      "\t+ Original Source:   mutch,”\n",
      "\t+ Reference Source:  much\n",
      "\t+ System Correction: much\n",
      "\n",
      "------------------- CHECK #26 -------------------\n",
      "\t+ Original Source:   con\n",
      "\t+ Reference Source:  can\n",
      "\t+ System Correction: coon\n",
      "\n",
      "------------------- CHECK #27 -------------------\n",
      "\t+ Original Source:   haerd\n",
      "\t+ Reference Source:  heard\n",
      "\t+ System Correction: heard\n",
      "\n",
      "------------------- CHECK #28 -------------------\n",
      "\t+ Original Source:   anye\n",
      "\t+ Reference Source:  any\n",
      "\t+ System Correction: any\n",
      "\n",
      "------------------- CHECK #29 -------------------\n",
      "\t+ Original Source:   frieends\n",
      "\t+ Reference Source:  friends\n",
      "\t+ System Correction: friends\n",
      "\n",
      "------------------- CHECK #30 -------------------\n",
      "\t+ Original Source:   hase\n",
      "\t+ Reference Source:  has\n",
      "\t+ System Correction: has\n",
      "\n",
      "------------------- CHECK #31 -------------------\n",
      "\t+ Original Source:   bene\n",
      "\t+ Reference Source:  be\n",
      "\t+ System Correction: be\n",
      "\n",
      "------------------- CHECK #32 -------------------\n",
      "\t+ Original Source:   professoor\n",
      "\t+ Reference Source:  professor\n",
      "\t+ System Correction: professor\n",
      "\n",
      "------------------- CHECK #33 -------------------\n",
      "\t+ Original Source:   universitiy\n",
      "\t+ Reference Source:  university\n",
      "\t+ System Correction: university\n",
      "\n",
      "------------------- CHECK #34 -------------------\n",
      "\t+ Original Source:   malnutrision\n",
      "\t+ Reference Source:  malnutrition\n",
      "\t+ System Correction: malnutrition\n",
      "\n",
      "------------------- CHECK #35 -------------------\n",
      "\t+ Original Source:   deit,\n",
      "\t+ Reference Source:  diet\n",
      "\t+ System Correction: diet\n",
      "\n",
      "------------------- CHECK #36 -------------------\n",
      "\t+ Original Source:   strongr?”\n",
      "\t+ Reference Source:  stronger\n",
      "\t+ System Correction: stronger\n",
      "\n",
      "------------------- CHECK #37 -------------------\n",
      "\t+ Original Source:   existental\n",
      "\t+ Reference Source:  existential\n",
      "\t+ System Correction: existential\n",
      "\n",
      "------------------- CHECK #38 -------------------\n",
      "\t+ Original Source:   dnner\n",
      "\t+ Reference Source:  dinner\n",
      "\t+ System Correction: dinner\n",
      "\n",
      "------------------- CHECK #39 -------------------\n",
      "\t+ Original Source:   evening\n",
      "\t+ Reference Source:  evening\n",
      "\t+ System Correction: weening\n",
      "\n",
      "------------------- CHECK #40 -------------------\n",
      "\t+ Original Source:   epcenter\n",
      "\t+ Reference Source:  epicenter\n",
      "\t+ System Correction: epicenter\n",
      "\n",
      "------------------- CHECK #41 -------------------\n",
      "\t+ Original Source:   prevous\n",
      "\t+ Reference Source:  previous\n",
      "\t+ System Correction: previous\n",
      "\n",
      "------------------- CHECK #42 -------------------\n",
      "\t+ Original Source:   generaions\n",
      "\t+ Reference Source:  generations\n",
      "\t+ System Correction: generations\n",
      "\n",
      "**************************\n",
      "P = 0.9523809523809523\tR = 0.9523809523809523\n"
     ]
    }
   ],
   "source": [
    "# Called the corpus.words from NLTK as the englishvoc.txt don't contain all the lexicons for the spelling checker program\n",
    "# nltk.corpus.words.words() is a list of more than 240.000 English lexicons\n",
    "start_spelling_checker(original_tokens, ref_tokens, nltk.corpus.words.words())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
